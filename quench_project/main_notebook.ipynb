{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning exam, Andrea Milici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.DataExplorator import DataExplorator\n",
    "from data.DataLoader import DataLoader\n",
    "from data.torch_Dataset import torch_Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.conv import output_dim\n",
    "from utils.model_utils import count_parameters,reset_weights\n",
    "from utils.seed import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset, di default lo converte in (N,24,15,15) e ritorna un dataframe pandas\n",
    "dataloader = DataLoader(\"/auto_atlas/atlas/atlas_gen_fs/amilici/other_samples/ml_exam_dataset/data_quench.json\") # transform_to_2d= True di default\n",
    "data= dataloader.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>quench</th>\n",
       "      <th>num_quench</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[[22.582478486789327, 22.904808739314365, 22....</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 18, 'pixel': [2, 1], 'temp': 25.1909...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[[22.0373877189044, 18.636944579237863, 21.76...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 8, 'pixel': [13, 6], 'temp': 25.0312...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[[21.411886934355916, 23.16618378647687, 20.1...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 14, 'pixel': [7, 7], 'temp': 24.8999...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[[21.276068279076245, 20.97700816985905, 21.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 8, 'pixel': [13, 6], 'temp': 24.4713...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[[21.69330864976891, 19.39115826850467, 20.55...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 15, 'pixel': [10, 13], 'temp': 25.69...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>[[[21.740674455238747, 19.547169172311275, 20....</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 11, 'pixel': [8, 4], 'temp': 24.7140...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2966</th>\n",
       "      <td>[[[19.118253683343585, 21.739494679368477, 21....</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 5, 'pixel': [9, 5], 'temp': 24.73965...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>[[[20.366059839869674, 21.313603736769316, 20....</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 18, 'pixel': [6, 13], 'temp': 24.451...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>[[[20.94590826562088, 22.970579566231, 21.8976...</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 7, 'pixel': [9, 10], 'temp': 25.7973...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>[[[21.911561011807102, 23.134242954820195, 20....</td>\n",
       "      <td>1</td>\n",
       "      <td>[{'step': 10, 'pixel': [3, 9], 'temp': 24.2369...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  label  \\\n",
       "8     [[[22.582478486789327, 22.904808739314365, 22....      1   \n",
       "9     [[[22.0373877189044, 18.636944579237863, 21.76...      1   \n",
       "20    [[[21.411886934355916, 23.16618378647687, 20.1...      1   \n",
       "21    [[[21.276068279076245, 20.97700816985905, 21.2...      1   \n",
       "23    [[[21.69330864976891, 19.39115826850467, 20.55...      1   \n",
       "...                                                 ...    ...   \n",
       "2946  [[[21.740674455238747, 19.547169172311275, 20....      1   \n",
       "2966  [[[19.118253683343585, 21.739494679368477, 21....      1   \n",
       "2977  [[[20.366059839869674, 21.313603736769316, 20....      1   \n",
       "2980  [[[20.94590826562088, 22.970579566231, 21.8976...      1   \n",
       "2989  [[[21.911561011807102, 23.134242954820195, 20....      1   \n",
       "\n",
       "                                                 quench  num_quench  \n",
       "8     [{'step': 18, 'pixel': [2, 1], 'temp': 25.1909...         1.0  \n",
       "9     [{'step': 8, 'pixel': [13, 6], 'temp': 25.0312...         7.0  \n",
       "20    [{'step': 14, 'pixel': [7, 7], 'temp': 24.8999...         1.0  \n",
       "21    [{'step': 8, 'pixel': [13, 6], 'temp': 24.4713...         2.0  \n",
       "23    [{'step': 15, 'pixel': [10, 13], 'temp': 25.69...         1.0  \n",
       "...                                                 ...         ...  \n",
       "2946  [{'step': 11, 'pixel': [8, 4], 'temp': 24.7140...         4.0  \n",
       "2966  [{'step': 5, 'pixel': [9, 5], 'temp': 24.73965...         3.0  \n",
       "2977  [{'step': 18, 'pixel': [6, 13], 'temp': 24.451...         1.0  \n",
       "2980  [{'step': 7, 'pixel': [9, 10], 'temp': 25.7973...         5.0  \n",
       "2989  [{'step': 10, 'pixel': [3, 9], 'temp': 24.2369...         6.0  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.label==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration = DataExplorator(data) \n",
    "# Vediamo una sequenza di heatmap con label == 1\n",
    "exploration.visualize_sequence(quenched=True,debug=True)\n",
    "# Vediamo una sequenza di heatmap con label == 0\n",
    "exploration.visualize_sequence(quenched=False)\n",
    "exploration.more_data_exploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: Normalizzazione dei dati\n",
    "# Trova min e max globali su tutto il dataset\n",
    "\n",
    "all_values = np.concatenate([sequence.flatten() for sequence in data.sequence])\n",
    "global_min = np.min(all_values)\n",
    "global_max = np.max(all_values)\n",
    "denom = global_max - global_min\n",
    "data.sequence = (data.sequence- global_min)/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "downsapmping = False\n",
    "\n",
    "if downsapmping:\n",
    "    # Esegui il downsampling per bilanciare le classi\n",
    "    print(\"Eseguo il downsampling per bilanciare le classi\")\n",
    "    # Supponiamo che df sia il tuo DataFrame con una colonna 'label'\n",
    "    class0 = data[data.label == 0]\n",
    "    class1 = data[data.label == 1]\n",
    "\n",
    "    # Sottocampiona la classe 0\n",
    "    class0_downsampled = resample(class0, \n",
    "                                       replace=False,     # no replacement\n",
    "                                       n_samples=300,     # come la classe minoritaria\n",
    "                                       random_state=42)\n",
    "\n",
    "    # Combina le due classi\n",
    "    df_balanced = pd.concat([class0_downsampled, class1], ignore_index=True)\n",
    "\n",
    "    # Shuffle (facoltativo ma consigliato)\n",
    "    data = df_balanced.sample(frac=1, random_state=42).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from data.torch_Dataset import torch_Dataset\n",
    "# Creazione del dataset PyTorch\n",
    "full_dataset = torch_Dataset(data, grid_size=15)\n",
    "\n",
    "\n",
    "# Creazione del DataLoader\n",
    "\n",
    "# Train/test split\n",
    "test_ratio = 0.2\n",
    "test_size = int(len(full_dataset) * test_ratio)\n",
    "train_size = len(full_dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Creazione dei DataLoader per il training e il test\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "from models.CNN_3D import CNN_3D\n",
    "model = CNN_3D()\n",
    "\n",
    "# Definizione della loss\n",
    "pos_weight= torch.tensor([len(data[data.label==0])/len(data[data.label==1])])  # Peso per la classe positiva, da regolare in base al dataset\n",
    "print(\"Pos weight:\", pos_weight)\n",
    "\n",
    "loss_BCEWithLogits= nn.BCEWithLogitsLoss(pos_weight=None)  # Per classificazione binaria\n",
    "\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Parametri totali: {total_params}\")\n",
    "print(f\"Parametri trainabili: {trainable_params}\")\n",
    "\n",
    "#---Training---\n",
    "\n",
    "from train.trainer import TrainerCNN3D\n",
    "from train.trainer import Results\n",
    "\n",
    "trainer = TrainerCNN3D(model, loss_BCEWithLogits,optimizer='Adam',lr=1e-3, num_epochs=100, train_loader=train_loader ,val_loader=test_loader)\n",
    "results=trainer.run()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot_results(results,trainer):\n",
    "\n",
    "    # Estrai le metriche dai risultati\n",
    "    train_losses_per_batch = results.get_train_losses_per_batch()\n",
    "    train_losses = results.get_train_loss()\n",
    "    test_losses = results.get_test_loss()\n",
    "    test_accuracy_per_epoch = results.get_test_accuracy()\n",
    "    num_epochs = trainer.num_epochs\n",
    "\n",
    "\n",
    "    # Calcola il numero di batch per epoca\n",
    "    num_batches_per_epoch = len(train_losses_per_batch) // num_epochs\n",
    "\n",
    "    # Crea un asse x \"frazionario\" per le batch, spostato in modo che l'epoca 1 inizi da x=1\n",
    "    batch_x_axis = np.linspace(1, num_epochs, len(train_losses_per_batch))\n",
    "\n",
    "    # Asse x per le medie per epoca, da 1 a num_epochs inclusi\n",
    "    epoch_x_axis = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    # Plotting delle loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Loss per batch (sfumata)\n",
    "    plt.plot(batch_x_axis, np.array(train_losses_per_batch), label='Train Loss per Batch', alpha=0.3, color='orange')\n",
    "\n",
    "    # Loss medie per epoca\n",
    "    plt.plot(epoch_x_axis, train_losses, label='Avg Train Loss per Epoch', color='blue', linewidth=2)\n",
    "    plt.plot(epoch_x_axis, test_losses, label='Test Loss per Epoch', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0, max(max(train_losses), max(test_losses)) * 2)  # Imposta un limite superiore dinamico\n",
    "    plt.title('Training and Test Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epoch_x_axis, test_accuracy_per_epoch, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Test Accuracy per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "x_all = torch.stack([x for x, y in test_dataset])\n",
    "y_all = torch.stack([y for x, y in test_dataset])\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "with torch.no_grad():\n",
    "    logits = model(x_all)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= 0.5).float()\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Classificazione standard\n",
    "print(classification_report(y_all, preds))\n",
    "print(confusion_matrix(y_all, preds))\n",
    "\n",
    "# ROC AUC\n",
    "print(\"ROC AUC:\", roc_auc_score(y_all,probs))\n",
    "\n",
    "# Curve\n",
    "fpr, tpr, _ = roc_curve(y_all, probs)\n",
    "precision, recall, _ = precision_recall_curve(y_all, probs)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# ROC\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# PR Curve\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(recall, precision, label='PR Curve', color='green')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "from utils.plot import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(y_all, preds, labels=[\"Unquenched\", \"Quenched\"])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function del VAE\n",
    "\n",
    "La funzione di loss di un VAE è composta da due termini:\n",
    "\n",
    "- **Errore di ricostruzione**: misura quanto bene il decoder riesce a ricostruire l'input a partire dal vettore latente.\n",
    "- **Div. di Kullback-Leibler (KL)**: regolarizza la distribuzione latente per essere vicina a una normale standard \\( \\mathcal{N}(0, I) \\).\n",
    "\n",
    "La formula completa è:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}} = \\underbrace{\\text{MSE}(x, \\hat{x})}_{\\text{ricostruzione}} + \\underbrace{D_{\\text{KL}}\\left( \\mathcal{N}(\\mu, \\sigma^2) \\,\\|\\, \\mathcal{N}(0, 1) \\right)}_{\\text{regolarizzazione}}\n",
    "$$\n",
    "\n",
    "Esplicitando il termine KL:\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}} = -\\frac{1}{2} \\sum_{i=1}^d \\left( 1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2 \\right)\n",
    "$$\n",
    "\n",
    "Dove:\n",
    "- \\( \\mu \\) e \\( \\sigma \\) sono i parametri della distribuzione latente \\( q(z|x) \\)\n",
    "- \\( d \\) è la dimensione dello spazio latente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.VAE import VAE\n",
    "from models.ConvAE import ConvAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_hat, mu, logvar,beta=1):\n",
    "    # Ricostruzione (mean squared error)\n",
    "    recon_loss = nn.functional.mse_loss(x_hat, x, reduction='mean')\n",
    "\n",
    "    # KL divergence (differenza tra z ~ q(z|x) e N(0,I))\n",
    "    kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return recon_loss + beta*kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cae_loss(x, x_hat):\n",
    "    # Ricostruzione (mean squared error)\n",
    "    return nn.functional.mse_loss(x_hat, x, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_quenched = data.label == 1\n",
    "is_unquenched = data.label == 0\n",
    "\n",
    "quenched_dataset = torch_Dataset(data[is_quenched], grid_size=15)\n",
    "unquenched_dataset = torch_Dataset(data[is_unquenched], grid_size=15)\n",
    "\n",
    "num_test_normal = 300  # Numero di sequenze normali per il test\n",
    "num_train= int((len(unquenched_dataset) - num_test_normal)*0.90)\n",
    "num_val = int((len(unquenched_dataset) - num_test_normal)*0.10)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "normal_train, normal_val, normal_test = random_split(unquenched_dataset, [num_train, num_val, num_test_normal])\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(normal_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(normal_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = quenched_dataset + normal_test\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Val size: {len(val_loader.dataset)}, Test size: {len(test_loader.dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.trainer import TrainerVAE\n",
    "\n",
    "model_vae = VAE(latent_dim=2,k=1)\n",
    "count_parameters(model_vae)\n",
    "\n",
    "trainer_vae = TrainerVAE(model_vae, vae_loss, optimizer='Adam', lr=1e-4, num_epochs=50, train_loader=train_loader, val_loader=val_loader)\n",
    "results_vae = trainer_vae.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_vae.get_val_loss(), label='Validation Loss', color='orange')\n",
    "plt.plot(results_vae.get_train_loss(), label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VAE Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"plots/vae_training_loss_{trainer_vae.num_epochs}_epochs_{trainer_vae.lr}_lr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score#\n",
    "from utils.plot import plot_latent_space_2d\n",
    "from utils.plot import plot_confusion_matrix\n",
    "\n",
    "\n",
    "def test_anomaly_detection(model, trainer,test_loader,model_name):\n",
    "    lr = trainer.lr\n",
    "    num_epochs = trainer.num_epochs\n",
    "    device= trainer.device\n",
    "\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    labels = []\n",
    "    mus = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            x_hat = model(x)\n",
    "            mu, logvar = model.encode(x)\n",
    "            \n",
    "            # Calcola loss di ricostruzione sommata per ogni esempio (batch element)\n",
    "            recon_loss = F.mse_loss(x_hat, x, reduction='none')\n",
    "            recon_loss = recon_loss.view(x.size(0), -1).sum(dim=1)  # somma su tutti i pixel/timepoint\n",
    "            scores.append(recon_loss.cpu())\n",
    "            labels.append(y)\n",
    "            mus.append(mu.cpu())\n",
    "\n",
    "    scores = torch.cat(scores).numpy()\n",
    "    labels = torch.cat(labels).numpy().reshape(-1)\n",
    "    mus = torch.cat(mus).numpy()\n",
    "    # Visualizzazione dello spazio latente\n",
    "    plot_latent_space_2d(mus,labels,lr,num_epochs, only_quenched=False)\n",
    "\n",
    "    # Histograms of scores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(scores[labels == 0], bins=40, alpha=0.5, label='Normal', color='blue')\n",
    "    plt.hist(scores[labels == 1], bins=40, alpha=0.5, label='Anomalous', color='red')\n",
    "    plt.xlabel('scores')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('VAE Reconstruction scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    mean_normal = scores[labels == 0].mean()\n",
    "    mean_anomalous = scores[labels == 1].mean()\n",
    "\n",
    "    plt.text(0.05, 0.95,f\"lr={lr}, epochs={num_epochs} \\nMean Normal: {mean_normal:.4f} \\nMean Anomalous: {mean_anomalous:.4f} \\nDistance: {abs(mean_normal - mean_anomalous):.4f}\",\n",
    "             transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
    "    plt.savefig(f\"plots/{model_name}_hist_scores_lr_{lr}_epochs_{num_epochs}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Calcolo ROC AUC\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    print(f\"ROC AUC: {auc:.4f}\")\n",
    "\n",
    "    # Calcolo ROC curve per trovare soglia ottimale (Youden’s J statistic)\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    youden_j = tpr - fpr\n",
    "    best_idx = np.argmax(youden_j)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    print(f\"Soglia ottimale (Youden's J): {best_threshold:.4f}\")\n",
    "\n",
    "    # Classificazione binaria con soglia ottimale\n",
    "    preds = (scores > best_threshold).astype(int)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.scatter(fpr[best_idx], tpr[best_idx], marker='o', color='red', label='Best threshold')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve per Anomaly Detection')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plot_confusion_matrix(labels, preds, labels=[\"Normal\", \"Anomalous\"],title=f\"{model_name}-lr_{lr}_epochs_{num_epochs}_confusion_matrix\")\n",
    "    \n",
    "    plt.show()\n",
    "    return best_threshold, auc, acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold, auc, acc, prec, rec, f1 = test_anomaly_detection(model_vae, trainer_vae,test_loader,'VAE')\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.trainer import TrainerConvAE\n",
    "model_cae = ConvAE(latent_dim=2,k=1)\n",
    "count_parameters(model_cae)\n",
    "\n",
    "trainer_cae = TrainerConvAE(model_cae, cae_loss, optimizer='Adam', lr=1e-4, num_epochs=50, train_loader=train_loader, val_loader=val_loader)\n",
    "results_cae = trainer_cae.run()\n",
    "# Plotting the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_cae.get_val_loss(), label='Validation Loss', color='orange')\n",
    "plt.plot(results_cae.get_train_loss(), label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ConvAE Training Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"plots/conv_ae_training_loss_{trainer_cae.num_epochs}_epochs_{trainer_cae.lr}_lr.png\")\n",
    "plt.show()\n",
    "best_threshold, auc, acc, prec, rec, f1 = test_anomaly_detection(model_cae, trainer_cae,test_loader,'ConvAE')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amilici_exam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
